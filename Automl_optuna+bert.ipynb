{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adarshblock/Automl-project/blob/main/Automl_optuna%2Bbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import sklearn.linear_model\n",
        "import sklearn.model_selection\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n",
        "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=2)\n",
        "\n",
        "# Preprocess data\n",
        "X = list(df_training['DESCRIPTION'])\n",
        "y = list(df_training['LABEL'])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=2018, test_size=0.20)\n",
        "\n",
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
        "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "# Create Torch Dataset\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = Dataset(X_val_tokenized, y_val)\n",
        "\n",
        "# Define Trainer parameters\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "def objective(trial: optuna.Trial):\n",
        "    model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=2)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='ade-test',\n",
        "        learning_rate=trial.suggest_loguniform('learning_rate', low=1e-6, high=1e-4),\n",
        "        weight_decay=trial.suggest_loguniform('weight_decay', 1e-6, 0.01),\n",
        "        num_train_epochs=trial.suggest_int('num_train_epochs', low = 2,high= 5),\n",
        "        per_device_train_batch_size=trial.suggest_categorical(\"per_device_train_batch_size\",[4,8,16]),\n",
        "        per_device_eval_batch_size=trial.suggest_categorical(\"per_device_eval_batch_size\", [4,8,16]),\n",
        "        warmup_ratio=trial.suggest_float(\"warmup_ration\",0.01,0.3),\n",
        "        adam_epsilon=trial.suggest_float(\"adam_epsilon\", 1e-12, 1e-8),\n",
        "        disable_tqdm=True\n",
        "    )\n",
        "    trainer = Trainer(model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "    )\n",
        "    result = trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "    # Define test trainer\n",
        "    test_trainer = Trainer(model)\n",
        "\n",
        "    # Make prediction\n",
        "    raw_pred, _, _ = test_trainer.predict(val_dataset)\n",
        "\n",
        "    # Preprocess raw predictions\n",
        "    y_pred = np.argmax(raw_pred, axis=1)\n",
        "\n",
        "    mcc = matthews_corrcoef(y_val, y_pred)\n",
        "\n",
        "    #return result.training_loss\n",
        "    return mcc\n",
        "\n",
        "# We want to maximize the MCC!\n",
        "sampler = TPESampler(seed=42)\n",
        "study = optuna.create_study(study_name='hyper-parameter-search', direction='maximize', sampler=sampler)\n",
        "# Optimize the objective using 15 different trials\n",
        "study.optimize(func=objective, n_trials=15, gc_after_trial=True)\n",
        "# Gives the best loss value\n",
        "print(study.best_value)\n",
        "# Gives the best hyperparameter values to get the best loss value print(study.best_params)\n",
        "# Return info about best Trial such as start and end datetime, hyperparameters\n",
        "print(study.best_trial)\n",
        "\n",
        "trial = study.best_trial\n",
        "print('MCC:{}'.format(trial.value))\n",
        "print('Best hyperparameters:{}'.format(trial.params))\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "best_model_to_save = model.module if hasattr(trial, 'module') else model  # Take care of distributed/parallel training\n",
        "best_model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"/content/drive/MyDrive/Best_Fine_tuned_model\"\n",
        "\n",
        "#load the model\n",
        "loaded_model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/Best_Fine_tuned_model/model_save')\n",
        "\n",
        "# Define test trainer\n",
        "test_trainer = Trainer(loaded_model)\n",
        "\n",
        "# Make prediction\n",
        "raw_pred, _, _ = test_trainer.predict(val_dataset)\n",
        "\n",
        "# Preprocess raw predictions\n",
        "y_pred = np.argmax(raw_pred, axis=1).flatten()\n",
        "\n",
        "true_label_testing = y_val\n",
        "\n",
        "predictions_test_flat = y_pred\n",
        "\n",
        "cr =(classification_report(true_label_testing, predictions_test_flat))\n",
        "print(cr)\n",
        "\n",
        "mcc = matthews_corrcoef(true_label_testing, predictions_test_flat)"
      ],
      "metadata": {
        "id": "UEhJJZGX8FQy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCasywWx0Q7zTFj37tqGDT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}